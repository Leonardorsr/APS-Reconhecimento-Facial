{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação das variáveis modelos\n",
    "\n",
    "eigenface = cv2.face.EigenFaceRecognizer_create(num_components = 50,threshold = 0)\n",
    "fisherFace = cv2.face.FisherFaceRecognizer_create(num_components = 50,threshold = 0)\n",
    "lbph = cv2.face.LBPHFaceRecognizer_create(threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fotos\\\\pessoa.1.1.jpg', 'fotos\\\\pessoa.1.10.jpg', 'fotos\\\\pessoa.1.11.jpg', 'fotos\\\\pessoa.1.12.jpg', 'fotos\\\\pessoa.1.13.jpg', 'fotos\\\\pessoa.1.14.jpg', 'fotos\\\\pessoa.1.15.jpg', 'fotos\\\\pessoa.1.16.jpg', 'fotos\\\\pessoa.1.17.jpg', 'fotos\\\\pessoa.1.18.jpg', 'fotos\\\\pessoa.1.19.jpg', 'fotos\\\\pessoa.1.2.jpg', 'fotos\\\\pessoa.1.20.jpg', 'fotos\\\\pessoa.1.21.jpg', 'fotos\\\\pessoa.1.22.jpg', 'fotos\\\\pessoa.1.23.jpg', 'fotos\\\\pessoa.1.24.jpg', 'fotos\\\\pessoa.1.25.jpg', 'fotos\\\\pessoa.1.3.jpg', 'fotos\\\\pessoa.1.4.jpg', 'fotos\\\\pessoa.1.5.jpg', 'fotos\\\\pessoa.1.6.jpg', 'fotos\\\\pessoa.1.7.jpg', 'fotos\\\\pessoa.1.8.jpg', 'fotos\\\\pessoa.1.9.jpg', 'fotos\\\\pessoa.2.1.jpg', 'fotos\\\\pessoa.2.10.jpg', 'fotos\\\\pessoa.2.11.jpg', 'fotos\\\\pessoa.2.12.jpg', 'fotos\\\\pessoa.2.13.jpg', 'fotos\\\\pessoa.2.14.jpg', 'fotos\\\\pessoa.2.15.jpg', 'fotos\\\\pessoa.2.16.jpg', 'fotos\\\\pessoa.2.17.jpg', 'fotos\\\\pessoa.2.18.jpg', 'fotos\\\\pessoa.2.19.jpg', 'fotos\\\\pessoa.2.2.jpg', 'fotos\\\\pessoa.2.20.jpg', 'fotos\\\\pessoa.2.21.jpg', 'fotos\\\\pessoa.2.22.jpg', 'fotos\\\\pessoa.2.23.jpg', 'fotos\\\\pessoa.2.24.jpg', 'fotos\\\\pessoa.2.25.jpg', 'fotos\\\\pessoa.2.3.jpg', 'fotos\\\\pessoa.2.4.jpg', 'fotos\\\\pessoa.2.5.jpg', 'fotos\\\\pessoa.2.6.jpg', 'fotos\\\\pessoa.2.7.jpg', 'fotos\\\\pessoa.2.8.jpg', 'fotos\\\\pessoa.2.9.jpg', 'fotos\\\\pessoa.3.1.jpg', 'fotos\\\\pessoa.3.10.jpg', 'fotos\\\\pessoa.3.11.jpg', 'fotos\\\\pessoa.3.12.jpg', 'fotos\\\\pessoa.3.13.jpg', 'fotos\\\\pessoa.3.14.jpg', 'fotos\\\\pessoa.3.15.jpg', 'fotos\\\\pessoa.3.16.jpg', 'fotos\\\\pessoa.3.17.jpg', 'fotos\\\\pessoa.3.18.jpg', 'fotos\\\\pessoa.3.19.jpg', 'fotos\\\\pessoa.3.2.jpg', 'fotos\\\\pessoa.3.20.jpg', 'fotos\\\\pessoa.3.21.jpg', 'fotos\\\\pessoa.3.22.jpg', 'fotos\\\\pessoa.3.23.jpg', 'fotos\\\\pessoa.3.24.jpg', 'fotos\\\\pessoa.3.25.jpg', 'fotos\\\\pessoa.3.3.jpg', 'fotos\\\\pessoa.3.4.jpg', 'fotos\\\\pessoa.3.5.jpg', 'fotos\\\\pessoa.3.6.jpg', 'fotos\\\\pessoa.3.7.jpg', 'fotos\\\\pessoa.3.8.jpg', 'fotos\\\\pessoa.3.9.jpg', 'fotos\\\\pessoa.4.1.jpg', 'fotos\\\\pessoa.4.10.jpg', 'fotos\\\\pessoa.4.11.jpg', 'fotos\\\\pessoa.4.12.jpg', 'fotos\\\\pessoa.4.13.jpg', 'fotos\\\\pessoa.4.14.jpg', 'fotos\\\\pessoa.4.15.jpg', 'fotos\\\\pessoa.4.16.jpg', 'fotos\\\\pessoa.4.17.jpg', 'fotos\\\\pessoa.4.18.jpg', 'fotos\\\\pessoa.4.19.jpg', 'fotos\\\\pessoa.4.2.jpg', 'fotos\\\\pessoa.4.20.jpg', 'fotos\\\\pessoa.4.21.jpg', 'fotos\\\\pessoa.4.22.jpg', 'fotos\\\\pessoa.4.23.jpg', 'fotos\\\\pessoa.4.24.jpg', 'fotos\\\\pessoa.4.25.jpg', 'fotos\\\\pessoa.4.3.jpg', 'fotos\\\\pessoa.4.4.jpg', 'fotos\\\\pessoa.4.5.jpg', 'fotos\\\\pessoa.4.6.jpg', 'fotos\\\\pessoa.4.7.jpg', 'fotos\\\\pessoa.4.8.jpg', 'fotos\\\\pessoa.4.9.jpg']\n"
     ]
    }
   ],
   "source": [
    "#Leitura das imagens coletadas para aprendizagem supervisionada\n",
    "\n",
    "def getImagemComId():\n",
    "  caminhos = [os.path.join('fotos',f) for f in os.listdir('fotos')]\n",
    "  print(caminhos)\n",
    "  faces = []\n",
    "  ids = []\n",
    "  for caminhoImagem in caminhos:\n",
    "    imagemFace = cv2.imread(caminhoImagem)\n",
    "    imagemFaceCinza = cv2.cvtColor(imagemFace, cv2.COLOR_BGR2GRAY)\n",
    "    id = int(os.path.split(caminhoImagem)[-1].split('.')[1])\n",
    "    #print(id)\n",
    "    ids.append(id)\n",
    "    faces.append(imagemFaceCinza)\n",
    "    #cv2.imshow(\"Face\", imagemFace)\n",
    "    #cv2.waitKey(10)\n",
    "  return np.array(ids), faces\n",
    "\n",
    "ids, faces = getImagemComId()\n",
    "#print(ids)\n",
    "#print(faces)ff67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento Eigenface concluido\n"
     ]
    }
   ],
   "source": [
    "#Treinamento dos modelos supervisionados\n",
    "\n",
    "#Eigenface\n",
    "eigenface.train(faces, ids)\n",
    "eigenface.write('Classificadores_de_Treino\\classificardorEigen.yml')\n",
    "print(\"Treinamento Eigenface concluido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento Fisherface concluido\n"
     ]
    }
   ],
   "source": [
    "#Fisherface\n",
    "fisherFace.train(faces, ids)\n",
    "fisherFace.write('Classificadores_de_Treino\\classificardorFisher.yml')\n",
    "print(\"Treinamento Fisherface concluido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento LBPH concluido\n"
     ]
    }
   ],
   "source": [
    "#LBPH\n",
    "lbph.train(faces,ids)\n",
    "lbph.write('Classificadores_de_Treino\\classificadorLBPH.yml')\n",
    "print(\"Treinamento LBPH concluido\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
